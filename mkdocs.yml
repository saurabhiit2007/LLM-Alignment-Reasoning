site_name: LLM RL Fine-tuning
site_description: Technical documentation and notes on RL methods for LLM fine-tuning
site_author: Saurabh Goyal
theme:
  name: material
  features:
    - mathjax

markdown_extensions:
  - admonition
  - toc:
      permalink: true
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.superfences
  - pymdownx.details

extra_javascript:
  - https://polyfill.io/v3/polyfill.min.js?features=es6
  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

nav:
  - Home: index.md
  - RLHF
    - RLHF Pipeline - Reward Modeling & Preference Data Collection: rlhf_pipeline.md
    - RL Optimization Methods:
        - Proximal Policy Optimization (PPO): ./alignment_methods/rlhf/rl_otimization_methods/ppo.md
        - Direct Policy Optimization (DPO): ./alignment_methods/rlhf/rl_otimization_methods/dpo.md
        - Group Relative Policy Optimization (GRPO): ./alignment_methods/rlhf/rl_otimization_methods/grpo.md
    - KL Penalty & Reward Hacking: kl_penalty_reward_hacking.md
  - Methods:
      - Proximal Policy Optimization (PPO): methods/ppo.md
      - Direct Policy Optimization (DPO): methods/dpo.md
      - Direct Rewards Policy Optimization (DRPO): methods/drpo.md
      - Group Relative Policy Optimization (GRPO): methods/grpo.md
  - Supporting Topics:
      - KL Penalty: supporting_topics/kl_penalty.md
      - Deepseek RL Fine-tuning: supporting_topics/deepseek_rl_finetuning.md
      - Rewards Hacking: supporting_topics/rewards_hacking.md
  - References: references.md
